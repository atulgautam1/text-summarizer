{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc945fe",
   "metadata": {},
   "source": [
    "# Text Summarization using web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dccafc4",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e69b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f31bb",
   "metadata": {},
   "source": [
    "### Scraping the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = urllib.request.urlopen(input())\n",
    "article = scraped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4a66b",
   "metadata": {},
   "source": [
    "### Removing Square Brackets and Extra Spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]',' ',article_text)\n",
    "article_text = re.sub(r'\\s+',' ',article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754d919",
   "metadata": {},
   "source": [
    "### Removing special characters and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "formatted_article_text = re.sub(r's+', ' ', formatted_article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe44b7a",
   "metadata": {},
   "source": [
    "### Convert text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e7dc3",
   "metadata": {},
   "source": [
    "### Finding weighted frequencies of occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11885ad",
   "metadata": {},
   "source": [
    "### Normalizing the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf738ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47621cc",
   "metadata": {},
   "source": [
    "### Calculating sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12408758",
   "metadata": {},
   "source": [
    "### Summary of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "summary_sentences = heapq.nlargest(15, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66e2dd",
   "metadata": {},
   "source": [
    "# Text Summarization by text input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096df5d",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = input(\"Enter the text:\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544adb27",
   "metadata": {},
   "source": [
    "### Tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = punctuation + '\\n'\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a73819",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = max(word_frequencies.values())\n",
    "max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b566892",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfca222",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f08293",
   "metadata": {},
   "source": [
    "### Word Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa004853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b65581",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bf191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "select_length = int(len(sentence_tokens)*0.3)\n",
    "select_length\n",
    "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366fc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = [word.text for word in summary]\n",
    "summary = ''.join(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ddcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e7fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
